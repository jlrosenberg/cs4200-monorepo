# Functional Dependency Discovery

For installation/setup/configuration information, see `README.md` in the project root

#### Performance Metrics

The main metric for performance that I used was execution time of the program from start to termination. I did this using the bash `time` command, so instead of running just `npm start` to run the script, I ran `time npm start`. This prints out three different durations - real, user, and system. All metrics that I report in this writeup are based on the `real` duration.

It's also worth noting that there is a nontrivial amount of time taken starting/ending the process and acquiring the DB connection, and querying the d - meaning that if I just report the overall time the script took to execute, the small queries will look like they take far longer than they should relative to the big ones. Additionally, this also is impacted by how long the `SELECT` statement actually takes to execute - which shouldn't be part of my evaluation of the algorithm.

Thus, I measure performance time as follows:

1. Run the script with the FD discovery algorithm commented out, and measure the time it takes for the script to start, establish a db connection, execute the `SELECT` statement, close the connection, and clean up the process afterwards.
2. Run the script again with the FD discovery algorithm, and measure the time this takes.
3. Subtract the overhead measured via step 1 from the time measured in step 2.

I believe this is the most accurate way to measure the running time of the FD discovery algorithm. It may not be 100% accurate, but it is far more accurate then just measuring the time it takes to execute the script, which involves several other time-intensive things.

### Running the script with a small input set

First, I ran my script on a small table that I created manually with the following sql queries:

```sql
create table sample1(a varchar(256), b varchar(256), c varchar(256), d varchar(256))

insert into sample1 values (1, 2, 3, 4), (1, 2, 3, 3), (4, 5, 4, 3)
```

| a   | b   | c   | d   |
| --- | --- | --- | --- |
| 1   | 2   | 3   | 4   |
| 1   | 2   | 3   | 3   |
| 4   | 5   | 4   | 3   |

Running the FD discovery algorithm on this table took about `0.131 seconds`, however I suspect the margin of error in execution times between runs makes this negligable.

It produced the following minimal FDs `[ 'a -> b', 'a -> c', 'b -> a', 'b -> c', 'c -> a', 'c -> b' ]`

By running the algorithm on such a small set, I was able to assert that all of the FDs generated by this were all valid, minimal, and that there were none missing that I expected to see.

### Running a large dataset with only a few columns

Next, I wanted to see how well the script could handle a large amount of data with only a few columns. To do this, I leveraged the existing imdb psql database already on my system from ps3. For this initial query, I decided to use the names table, and run the FD discovery algorithm on the results of the following query
`select nconst, birthyear, deathyear from names;`

This specific dataset had a couple of benefits:

- Once again, I know exactly what FDs I expect as output.
- It's large (9133200 records), so it should be a good performance test.

The algorithm took 30.862 seconds to execute, and produced the following (correct) output: `[ 'nconst -> birthyear', 'nconst -> deathyear' ]`.

### Running a dataset with a medium number of columns

Next, I tried to run the algorithm on a dataset with more columns. This is where I started to notice a downgrade in performance.

For the dataset, I used the following query:
`select * from principals join names on principals.nconst=names.nconst limit 5000;`

This dataset had 10 columns and 5000 rows, and the algorithm took 46.288 seconds to execute.

Out of curiosity, I ran the exact same query with a limit of 2500 instead, and this took around 28 seconds to execute. This indicated to me that the number of rows being analyzed affects runtime in a linear way, whereas the number of columns is exponential.

### Running a dataset with a large number of columns

Finally, I tried to run a dataset with 20 columns, and a mere 5000 rows.

To generate this dataset, I used the following query:
`select * from principals join names on principals.nconst=names.nconst join titles on titles.tconst = principals.tconst limit 5000;`

Unfortunately, my algorithm did not terminate, and after ~15 minutes I realized that it was unlikely to finish anytime soon based on how many levels were left to traverse. This confirmed my theory that the the number of columns effects the FD detection in an exponential way, which is confirmed in the tane algorithm white paper.

## Performance Impactors

One thing that may have heavily impacted performance is all of the string equality that was being performed. Since the majority of the rows in these datasets were stored as strings, in order to group them into subsets, I had to do string equality on the values - this is an expensive operation, much more than integer or boolean equality.
